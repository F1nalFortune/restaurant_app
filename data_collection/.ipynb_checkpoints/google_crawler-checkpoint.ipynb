{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "from functools import wraps\n",
    "from colorama import Fore\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "\n",
    "from requests import ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(ExceptionToCheck, tries=20, delay=3, backoff=2, logger=None):\n",
    "    \"\"\"Retry calling the decorated function using an exponential backoff.\n",
    "\n",
    "    :param ExceptionToCheck: the exception to check. may be a tuple of\n",
    "        exceptions to check\n",
    "    :type ExceptionToCheck: Exception or tuple\n",
    "    :param tries: number of times to try (not retry) before giving up\n",
    "    :type tries: int\n",
    "    :param delay: initial delay between retries in seconds\n",
    "    :type delay: int\n",
    "    :param backoff: backoff multiplier e.g. value of 2 will double the delay\n",
    "        each retry\n",
    "    :type backoff: int\n",
    "    :param logger: logger to use. If None, print\n",
    "    :type logger: logging.Logger instance\n",
    "    \"\"\"\n",
    "    def deco_retry(f):\n",
    "\n",
    "        @wraps(f)\n",
    "        def f_retry(*args, **kwargs):\n",
    "            mtries, mdelay = tries, delay\n",
    "            while mtries > 1:\n",
    "                try:\n",
    "                    return f(*args, **kwargs)\n",
    "                except ExceptionToCheck:\n",
    "                    msg = \"%s, Retrying in %d seconds...\" % (str(ExceptionToCheck), mdelay)\n",
    "                    if logger:\n",
    "                        #logger.exception(msg) # would print stack trace\n",
    "                        logger.warning(msg)\n",
    "                    else:\n",
    "                        print(msg)\n",
    "                    time.sleep(mdelay)\n",
    "                    mtries -= 1\n",
    "                    mdelay *= backoff\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "        return f_retry  # true decorator\n",
    "\n",
    "    return deco_retry  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe = pd.read_csv(\"./data/state_data.csv\", index_col=0)\n",
    "result_dataframe['Cities'] = result_dataframe['Cities'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "discover_dataframe = pd.DataFrame({})\n",
    "restaurant_dataframe = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe = result_dataframe.loc[result_dataframe['State']=='Connecticut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only 50 states of data\n",
    "updated_state_data = pd.read_csv(f\"./data/us_cities_states_counties.csv\", index_col=0)\n",
    "state=updated_state_data.loc[updated_state_data['st_long']=='Connecticut']\n",
    "list_2, list_1 = state.city.unique(), result_dataframe['Cities'][6]\n",
    "\n",
    "cities = np.setdiff1d(list_2,list_1)\n",
    "# yields the elements in `list_2` that are NOT in `list_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seleniumwire'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-428b21910f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseleniumwire\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m  \u001b[0;31m# Import from seleniumwire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seleniumwire'"
     ]
    }
   ],
   "source": [
    "from seleniumwire import webdriver  # Import from seleniumwire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleSpider(object):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "      Crawl Google search results\n",
    "\n",
    "      This class is used to crawl Google's search results using requests and BeautifulSoup.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.discover_dataframe = pd.DataFrame({})\n",
    "    self.restaurant_dataframe = pd.DataFrame({})\n",
    "\n",
    "  def search(self, state: str, cities: list) -> list:\n",
    "      \"\"\"\n",
    "        Search Google\n",
    "\n",
    "        Args:\n",
    "          query (str): The query to search for\n",
    "\n",
    "        Returns:\n",
    "          list: The search results\n",
    "      \"\"\"\n",
    "      counter = 20\n",
    "      state = state\n",
    "      cities = cities\n",
    "      #define query and link\n",
    "      for city in cities:\n",
    "        query = f\"{city}, {state} Restaurants\"\n",
    "        query = query.replace(\",\", \"%2C\").replace(\" \", \"+\").replace(\" \", \"%20\")\n",
    "        main_link = f\"https://www.google.com/search?q={query}\"\n",
    "\n",
    "      #MACBOOK DATA\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"no-sandbox\")\n",
    "        options.add_argument(\"disable-dev-shm-usage\")\n",
    "#         options.add_argument(\"headless\")\n",
    "        options.add_argument(f\"user-data-dir=/Users/brianphelps/Library/Application Support/Google/Chrome/__Thread__{counter}\")\n",
    "        driver = webdriver.Chrome(executable_path=\"/Users/brianphelps/Desktop/chromedriver\", chrome_options=options)\n",
    "  \n",
    "        ua = UserAgent()\n",
    "        headers = {'User-Agent': ua.chrome}\n",
    "\n",
    "        @retry((ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError), tries=20, delay=2, backoff=2)\n",
    "        def serve_link(link):\n",
    "          driver.get(link)\n",
    "        serve_link(main_link)\n",
    "\n",
    "        try:\n",
    "          source = driver.page_source\n",
    "          html = BeautifulSoup(source, 'lxml')\n",
    "          # TRY TO FIND MORE RESULTS BUTTON\n",
    "          try:\n",
    "            result = html.find(text='More places').parent.parent.parent['href']\n",
    "          except:\n",
    "            try:\n",
    "              result = html.find(text='More results').parent.parent.parent['href']\n",
    "            except:\n",
    "              result = html.find(text='View all').parent.parent.parent['href']\n",
    "          link = f\"https://www.google.com/{result}\"\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          print(f\"{city}, {state}\")\n",
    "          #try to find captcha box to see if google is blocking requests\n",
    "          try:\n",
    "            captcha = html.find('div', class_='recaptcha-checkbox-border')\n",
    "            #manually click captcha box\n",
    "            breakpoint()\n",
    "          except:\n",
    "            pass\n",
    "\n",
    "        # view all restaurants for location\n",
    "#         html.find(text='View all').parent.parent.parent.click()\n",
    "        breakpoint()\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".MXl0lf\"))).click()\n",
    "        #serve_link(link)\n",
    "        source = driver.page_source\n",
    "        html = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        time.sleep(3)\n",
    "        # grab 'discover more places' items\n",
    "        try:\n",
    "          more_places = html.find(text='Discover more places').parent.parent.parent.find('g-scrolling-carousel').div.div.find_next('div').parent\n",
    "          for place in more_places:\n",
    "            discover_title = place.find('g-inner-card').find_all('div')[-1].parent.div.text.strip()\n",
    "            discover_link = f\"google.com{place.a['href']}\"\n",
    "\n",
    "            temp_result = {\n",
    "              'title': discover_title,\n",
    "              'link': discover_link,\n",
    "              'city': [city],\n",
    "              'state': [state]\n",
    "            }\n",
    "            self.discover_dataframe = self.discover_dataframe.append(temp_result, ignore_index=True)\n",
    "        except:\n",
    "          pass\n",
    "\n",
    "        #try to click next button\n",
    "        restaurants_exist = True\n",
    "        while restaurants_exist:\n",
    "          try:\n",
    "            restaurant_html = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            restaurants = restaurant_html.find('div', class_='rl_full-list').div.div.div.div.find_next_siblings()[2].find_next('div').parent\n",
    "            #grab all restaurant info\n",
    "            restaurant_length = len(restaurants)\n",
    "          except:\n",
    "            restaurants_exist = False\n",
    "            break\n",
    "          if restaurants_exist==True:\n",
    "            for idx, restaurant in enumerate(restaurants):\n",
    "              try:\n",
    "                #SKIP OVER ADVERTISEMENTS\n",
    "                if \"Why this ad?\" in restaurant.a.div.find_next('div').find_next_siblings()[0].div.text:\n",
    "                  continue\n",
    "                try:\n",
    "                  temp_rest = pd.DataFrame()\n",
    "                  title = restaurant.a.div.find_next('div').find_next_siblings()[0].div.text\n",
    "                  print(title)\n",
    "                  # OPEN RESTAURANT DETAILS\n",
    "                  # RESTAURANT ITEM JS PATH\n",
    "\n",
    "                  driver.execute_script(f\"\"\"\n",
    "                  document.querySelector(\"#rl_ist0 > div.rl_tile-group > div.rlfl__tls.rl_tls > div:nth-child({idx+1}) > div > div.uMdZh.tIxNaf.mnr-c > div > a > div\").click()\n",
    "                  \"\"\")\n",
    "                  time.sleep(3)\n",
    "                  restaurant_html = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                  container = restaurant_html.find_all('span', text=title)[0].parent.parent.parent.parent.parent\n",
    "                except:\n",
    "                  container = np.nan\n",
    "                  pass\n",
    "\n",
    "\n",
    "                try:\n",
    "                  star_count = container.find('g-review-stars').parent.span.text\n",
    "                  print(Fore.GREEN + f\"Star Count: {star_count}\")\n",
    "                except:\n",
    "                  star_count = np.nan\n",
    "                  print(Fore.GREEN + f\"Star Count Error\")\n",
    "\n",
    "                try:\n",
    "                  review_count = container.find('g-review-stars').parent.find_all('span')[3].span.a.text\n",
    "                  print(Fore.GREEN + f\"Review Count: {review_count}\")\n",
    "                except:\n",
    "                  review_count = np.nan\n",
    "                  print(Fore.GREEN + f\"Review Count Error\")\n",
    "\n",
    "                try:\n",
    "                  attrs = []\n",
    "                  attributes = container.find_all('div', class_='TLYLSe')[1].div.find_all('span')\n",
    "                  for attr in attributes:\n",
    "                    attrs.append(attr.text.strip())\n",
    "                  print(Fore.GREEN + f\"Attributes: {attrs}\")\n",
    "                  attributes = attrs\n",
    "                except Exception as e:\n",
    "                  attributes = np.nan\n",
    "                  print(e)\n",
    "                  print(Fore.RED + f\"Attributes Error\")\n",
    "\n",
    "\n",
    "                try:\n",
    "                  address = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/location/location:address\"}).div.div.find_all('span')[1].text\n",
    "                  print(Fore.GREEN + \"Address\")\n",
    "                except:\n",
    "                  address = np.nan\n",
    "                  print(Fore.GREEN + \"Address\")\n",
    "\n",
    "                try:\n",
    "                  hours = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/location/location:hours\"}).find_next('table').find_all('tr')\n",
    "                  hours_dict ={}\n",
    "                  for val in hours:\n",
    "                    hours_dict[val.find_all('td')[0].text.strip()]=val.find_all('td')[1].text.strip()\n",
    "                  hours = hours_dict\n",
    "                  print(Fore.GREEN + \"Hours\")\n",
    "                except:\n",
    "                  hours = np.nan\n",
    "                  print(Fore.RED + \"Hours Error\")\n",
    "\n",
    "                try:\n",
    "                  menu = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/local:menu\"}).find_next('a')['href']\n",
    "                  print(Fore.GREEN + \"Menu\")\n",
    "                except:\n",
    "                  menu = np.nan\n",
    "                  print(Fore.RED + \"Menu\")\n",
    "\n",
    "                try:\n",
    "                  phone = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/collection/knowledge_panels/has_phone:phone\"}).div.div.find_all('span')[1].span.text.strip()\n",
    "                  print(Fore.GREEN + \"Phone\")\n",
    "                except:\n",
    "                  phone = np.nan\n",
    "                  print(Fore.RED + \"Phone\")\n",
    "\n",
    "                try:\n",
    "                  website = container.find('a', text='Website')['href']\n",
    "                  print(Fore.GREEN + \"Website\")\n",
    "                except:\n",
    "                  website = np.nan\n",
    "                  print(Fore.RED + \"Website\")\n",
    "                #try to find pictures\n",
    "\n",
    "                try:\n",
    "                  # define pictures link\n",
    "                  pic_link = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/location/location:media\"}).div.a['href']\n",
    "                  pic_link = f\"https://www.google.com{pic_link}\"\n",
    "                  print(Fore.GREEN + f\"Picture Link: {pic_link}\")\n",
    "\n",
    "                  # grab pictures\n",
    "      #               serve_link(pic_link)\n",
    "                  # grab pictures that owner uploaded\n",
    "                  pic_options = webdriver.ChromeOptions()\n",
    "                  pic_options.add_argument(\"no-sandbox\")\n",
    "                  pic_options.add_argument(\"disable-dev-shm-usage\")\n",
    "#                     pic_options.add_argument(\"headless\")\n",
    "                  pic_options.add_argument(f\"user-data-dir=/Users/brianphelps/Library/Application Support/Google/Chrome/__Picture__{counter}\")\n",
    "                  pic_driver = webdriver.Chrome(executable_path=\"/Users/brianphelps/Desktop/chromedriver\", chrome_options=pic_options)\n",
    "\n",
    "\n",
    "\n",
    "                  @retry((ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError), tries=20, delay=2, backoff=2)\n",
    "                  def serve_pics(link):\n",
    "                    pic_driver.get(link)\n",
    "                  serve_pics(pic_link)\n",
    "                  time.sleep(10)\n",
    "                  pic_driver.execute_script(\"\"\"\n",
    "                  var divTags = document.getElementsByTagName(\"div\");\n",
    "                  var searchText = \"By owner\";\n",
    "                  var found;\n",
    "\n",
    "                  for (var i = 0; i < divTags.length; i++) {\n",
    "                    if (divTags[i].textContent == searchText) {\n",
    "                      found = divTags[i];\n",
    "                      found.click();\n",
    "                      break;\n",
    "                    }\n",
    "                  }\n",
    "                  \"\"\")\n",
    "                  for y in range(5):\n",
    "                    pic_driver.execute_script(\"\"\"\n",
    "                    var objDiv = document.querySelector(\"#gallery > div.widget-pane.widget-pane-visible > div.widget-pane-content.scrollable-y > div > div > div.section-layout.section-scrollbox.scrollable-y.scrollable-show\");\n",
    "                    var height = objDiv.scrollHeight;\n",
    "                    objDiv.scrollTop = objDiv.scrollHeight;\n",
    "                    \"\"\")\n",
    "                    time.sleep(3)\n",
    "                  # parse picture html\n",
    "                  pics_source = pic_driver.page_source\n",
    "                  pics_html = BeautifulSoup(pics_source, 'lxml')\n",
    "\n",
    "                  pictures = pics_html.find('div', class_='section-layout section-scrollbox scrollable-y scrollable-show').div\n",
    "                  pic_links = []\n",
    "                  for pic in pictures:\n",
    "                    try:\n",
    "                      pic_link = pic.div.a.div.div['style']\n",
    "                      pic_link = pic_link.split(\"background-image: url\")[1].replace(\"(\", \"\").replace(\")\", \"\").replace(\";\", \"\").replace('\"', '')\n",
    "                      pic_links.append(pic_link)\n",
    "                    except:\n",
    "                      pass\n",
    "                  pictures = pic_links\n",
    "                  print(Fore.GREEN + \"Pictures\")\n",
    "                except Exception as e:\n",
    "                  print(\"Picture Error\")\n",
    "                  print(e)\n",
    "                  pictures = np.nan\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                temp_rest = pd.DataFrame({\n",
    "                  'title': [title],\n",
    "                  'star_count': [star_count],\n",
    "                  'review_count': [review_count],\n",
    "                  'attributes': [attributes],\n",
    "                  'address': [address],\n",
    "                  'hours': [hours],\n",
    "                  'menu': [menu],\n",
    "                  'phone': [phone],\n",
    "                  'website':[website],\n",
    "                  'pictures': [pictures],\n",
    "                  'city': [city],\n",
    "                  'state': [state]\n",
    "                })\n",
    "\n",
    "                self.restaurant_dataframe = self.restaurant_dataframe.append(temp_rest, ignore_index=True)\n",
    "#                 breakpoint()\n",
    "                pic_driver.quit()\n",
    "              except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            if len(self.restaurant_dataframe)%200==0:\n",
    "              counter+=1\n",
    "            try:\n",
    "              next_button = restaurant_html.find('a', attrs={\"id\":\"pnnext\"})['href']\n",
    "              next_link = f\"https://www.google.com{next_button}\"\n",
    "              time.sleep(10)\n",
    "#               restaurant_html.find('a', attrs={\"id\":\"pnnext\"}).click()\n",
    "              WebDriverWait(driver, 5).until(\n",
    "                  EC.presence_of_element_located((By.ID, \"pnnext\"))\n",
    "              ).click()\n",
    "            except:\n",
    "      #         breakpoint()\n",
    "              restaurants_exist = False\n",
    "        self.discover_dataframe.to_csv(\"./discover_ct.csv\")\n",
    "        self.restaurant_dataframe.to_csv(\"./ct_restaurants.csv\")\n",
    "        driver.quit()\n",
    "      return self.discover_dataframe, self.restaurant_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-d750d73409bf>:37: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(executable_path=\"/Users/brianphelps/Desktop/chromedriver\", chrome_options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-58-d750d73409bf>\u001b[0m(73)\u001b[0;36msearch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     71 \u001b[0;31m\u001b[0;31m#         html.find(text='View all').parent.parent.parent.click()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     72 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m        \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".MXl0lf .mtqGb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m\u001b[0;31m#         WebDriverWait(driver, 20).until(EC.element_to_be_clickable())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     75 \u001b[0;31m        \u001b[0;31m#serve_link(link)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  WebDriverWait(browser, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".MXl0lf .mtqGb\"))).click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'browser' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".MXl0lf .mtqGb\"))).click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** selenium.common.exceptions.TimeoutException: Message:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".MXl0lf\"))).click()\n"
     ]
    }
   ],
   "source": [
    "spider = GoogleSpider()\n",
    "disc, rest = spider.search(state='Connecticut', cities=list(cities)[8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cities).index('Barkhamsted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Barkhamsted, Connecticut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
