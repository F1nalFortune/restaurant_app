{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "from functools import wraps\n",
    "from colorama import Fore\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "\n",
    "from requests import ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(ExceptionToCheck, tries=20, delay=3, backoff=2, logger=None):\n",
    "    \"\"\"Retry calling the decorated function using an exponential backoff.\n",
    "\n",
    "    :param ExceptionToCheck: the exception to check. may be a tuple of\n",
    "        exceptions to check\n",
    "    :type ExceptionToCheck: Exception or tuple\n",
    "    :param tries: number of times to try (not retry) before giving up\n",
    "    :type tries: int\n",
    "    :param delay: initial delay between retries in seconds\n",
    "    :type delay: int\n",
    "    :param backoff: backoff multiplier e.g. value of 2 will double the delay\n",
    "        each retry\n",
    "    :type backoff: int\n",
    "    :param logger: logger to use. If None, print\n",
    "    :type logger: logging.Logger instance\n",
    "    \"\"\"\n",
    "    def deco_retry(f):\n",
    "\n",
    "        @wraps(f)\n",
    "        def f_retry(*args, **kwargs):\n",
    "            mtries, mdelay = tries, delay\n",
    "            while mtries > 1:\n",
    "                try:\n",
    "                    return f(*args, **kwargs)\n",
    "                except ExceptionToCheck:\n",
    "                    msg = \"%s, Retrying in %d seconds...\" % (str(ExceptionToCheck), mdelay)\n",
    "                    if logger:\n",
    "                        #logger.exception(msg) # would print stack trace\n",
    "                        logger.warning(msg)\n",
    "                    else:\n",
    "                        print(msg)\n",
    "                    time.sleep(mdelay)\n",
    "                    mtries -= 1\n",
    "                    mdelay *= backoff\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "        return f_retry  # true decorator\n",
    "\n",
    "    return deco_retry  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe = pd.read_csv(\"./data/state_data.csv\", index_col=0)\n",
    "result_dataframe['Cities'] = result_dataframe['Cities'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "discover_dataframe = pd.DataFrame({})\n",
    "restaurant_dataframe = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe = result_dataframe.loc[result_dataframe['State']=='Connecticut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only 50 states of data\n",
    "updated_state_data = pd.read_csv(f\"./data/us_cities_states_counties.csv\", index_col=0)\n",
    "state=updated_state_data.loc[updated_state_data['st_long']=='Connecticut']\n",
    "list_2, list_1 = state.city.unique(), result_dataframe['Cities'][6]\n",
    "\n",
    "cities = np.setdiff1d(list_2,list_1)\n",
    "# yields the elements in `list_2` that are NOT in `list_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleSpider(object):\n",
    "  def __init__(self, state, cities):\n",
    "    \"\"\"\n",
    "      Crawl Google search results\n",
    "\n",
    "      This class is used to crawl Google's search results using requests and BeautifulSoup.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.discover_dataframe = pd.DataFrame({})\n",
    "    self.restaurant_dataframe = pd.DataFrame({})\n",
    "    self.state = state\n",
    "    self.cities = cities\n",
    "    self.counter=0\n",
    "    \n",
    "  def __driver(self, thread_name):\n",
    "    \"\"\"\n",
    "      Create Selenium Driver\n",
    "      \n",
    "      Params:\n",
    "        counter: str\n",
    "        changes user data directory every 200th iteration\n",
    "        eg. counter=counter\n",
    "    \"\"\"\n",
    "    #MACBOOK DATA\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"no-sandbox\")\n",
    "    options.add_argument(\"disable-dev-shm-usage\")\n",
    "    #options.add_argument(\"headless\")\n",
    "    options.add_argument(f\"user-data-dir=/Users/brianphelps/Library/Application Support/Google/Chrome/{thread_name}{self.counter}\")\n",
    "    driver = webdriver.Chrome(executable_path=\"/Users/brianphelps/Desktop/chromedriver\", chrome_options=options)\n",
    "    \n",
    "    return driver\n",
    "    \n",
    "    \n",
    "  def __main_places(self, html):\n",
    "    \"\"\"\n",
    "      Crawl Featured Places from Search\n",
    "      \n",
    "      This grabs links and lists of featured spots.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      more_places = html.find(text='Discover more places').parent.parent.parent.find('g-scrolling-carousel').div.div.find_next('div').parent\n",
    "      for place in more_places:\n",
    "        discover_title = place.find('g-inner-card').find_all('div')[-1].parent.div.text.strip()\n",
    "        discover_link = f\"google.com{place.a['href']}\"\n",
    "\n",
    "        temp_result = {\n",
    "          'title': discover_title,\n",
    "          'link': discover_link,\n",
    "          'city': [self.city],\n",
    "          'state': [self.state]\n",
    "        }\n",
    "        self.discover_dataframe = self.discover_dataframe.append(temp_result, ignore_index=True)\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "  def __picture_data(self,restaurant):\n",
    "    try:\n",
    "      restaurant_html = BeautifulSoup(self.restaurant_driver.page_source, 'lxml')\n",
    "      # define pictures link\n",
    "      pic_link = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/location/location:media\"}).div.a['href']\n",
    "      pic_link = f\"https://www.google.com{pic_link}\"\n",
    "      print(Fore.GREEN + f\"Picture Link: {pic_link}\")\n",
    "\n",
    "\n",
    "      # grab pictures that owner uploaded\n",
    "      pic_driver = self.__driver('__Picture__')\n",
    "\n",
    "\n",
    "\n",
    "      @retry((ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError), tries=20, delay=2, backoff=2)\n",
    "      def serve_pics(link):\n",
    "        pic_driver.get(link)\n",
    "      serve_pics(pic_link)\n",
    "      time.sleep(10)\n",
    "      pic_driver.execute_script(\"\"\"\n",
    "      var divTags = document.getElementsByTagName(\"div\");\n",
    "      var searchText = \"By owner\";\n",
    "      var found;\n",
    "\n",
    "      for (var i = 0; i < divTags.length; i++) {\n",
    "        if (divTags[i].textContent == searchText) {\n",
    "          found = divTags[i];\n",
    "          found.click();\n",
    "          break;\n",
    "        }\n",
    "      }\n",
    "      \"\"\")\n",
    "      for y in range(5):\n",
    "        pic_driver.execute_script(\"\"\"\n",
    "        var objDiv = document.querySelector(\"#gallery > div.widget-pane.widget-pane-visible > div.widget-pane-content.scrollable-y > div > div > div.section-layout.section-scrollbox.scrollable-y.scrollable-show\");\n",
    "        var height = objDiv.scrollHeight;\n",
    "        objDiv.scrollTop = objDiv.scrollHeight;\n",
    "        \"\"\")\n",
    "        time.sleep(3)\n",
    "      # parse picture html\n",
    "      pics_source = pic_driver.page_source\n",
    "      pics_html = BeautifulSoup(pics_source, 'lxml')\n",
    "\n",
    "      pictures = pics_html.find('div', class_='section-layout section-scrollbox scrollable-y scrollable-show').div\n",
    "      pic_links = []\n",
    "      for pic in pictures:\n",
    "        try:\n",
    "          pic_link = pic.div.a.div.div['style']\n",
    "          pic_link = pic_link.split(\"background-image: url\")[1].replace(\"(\", \"\").replace(\")\", \"\").replace(\";\", \"\").replace('\"', '')\n",
    "          pic_links.append(pic_link)\n",
    "        except:\n",
    "          pass\n",
    "      pictures = pic_links\n",
    "      print(Fore.GREEN + \"Pictures\")\n",
    "    except Exception as e:\n",
    "      print(\"Pictures Error\")\n",
    "      print(e)\n",
    "      pictures = np.nan\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    try:\n",
    "      pic_driver.quit()\n",
    "    except:\n",
    "      pass\n",
    "    return pictures\n",
    "\n",
    "    \n",
    "  def __restaurant_data(self,idx, restaurant):\n",
    "    try:\n",
    "      title = restaurant.a.div.find_next('div').find_next_siblings()[0].div.text\n",
    "      print(title)\n",
    "\n",
    "      # RESTAURANT ITEM JS PATH\n",
    "\n",
    "      self.restaurant_driver.execute_script(f\"\"\"\n",
    "      document.querySelector(\"#rl_ist0 > div.rl_tile-group > div.rlfl__tls.rl_tls > div:nth-child({idx+1}) > div > div.uMdZh.tIxNaf.mnr-c > div > a > div\").click()\n",
    "      \"\"\")\n",
    "      time.sleep(3)\n",
    "      restaurant_html = BeautifulSoup(self.restaurant_driver.page_source, 'lxml')\n",
    "      container = restaurant_html.find_all('span', text=title)[0].parent.parent.parent.parent.parent\n",
    "    except:\n",
    "      container = np.nan\n",
    "      title = np.nan\n",
    "      pass\n",
    "\n",
    "\n",
    "    try:\n",
    "      star_count = container.find('g-review-stars').parent.span.text\n",
    "      print(Fore.GREEN + f\"Star Count: {star_count}\")\n",
    "    except:\n",
    "      star_count = np.nan\n",
    "      print(Fore.GREEN + f\"Star Count Error\")\n",
    "\n",
    "    try:\n",
    "      review_count = container.find('g-review-stars').parent.find_all('span')[3].span.a.text\n",
    "      print(Fore.GREEN + f\"Review Count: {review_count}\")\n",
    "    except:\n",
    "      review_count = np.nan\n",
    "      print(Fore.GREEN + f\"Review Count Error\")\n",
    "\n",
    "    try:\n",
    "      attrs = []\n",
    "      attributes = container.find_all('div', class_='TLYLSe')[1].div.find_all('span')\n",
    "      for attr in attributes:\n",
    "        attrs.append(attr.text.strip())\n",
    "      print(Fore.GREEN + f\"Attributes: {attrs}\")\n",
    "      attributes = attrs\n",
    "    except Exception as e:\n",
    "      attributes = np.nan\n",
    "      print(e)\n",
    "      print(Fore.RED + f\"Attributes Error\")\n",
    "\n",
    "\n",
    "    try:\n",
    "      address = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/location/location:address\"}).div.div.find_all('span')[1].text\n",
    "      print(Fore.GREEN + \"Address\")\n",
    "    except:\n",
    "      address = np.nan\n",
    "      print(Fore.GREEN + \"Address\")\n",
    "\n",
    "    try:\n",
    "      hours = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/location/location:hours\"}).find_next('table').find_all('tr')\n",
    "      hours_dict ={}\n",
    "      for val in hours:\n",
    "        hours_dict[val.find_all('td')[0].text.strip()]=val.find_all('td')[1].text.strip()\n",
    "      hours = hours_dict\n",
    "      print(Fore.GREEN + \"Hours\")\n",
    "    except:\n",
    "      hours = np.nan\n",
    "      print(Fore.RED + \"Hours Error\")\n",
    "\n",
    "    try:\n",
    "      menu = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/local:menu\"}).find_next('a')['href']\n",
    "      print(Fore.GREEN + \"Menu\")\n",
    "    except:\n",
    "      menu = np.nan\n",
    "      print(Fore.RED + \"Menu\")\n",
    "\n",
    "    try:\n",
    "      phone = restaurant_html.find('div', attrs={\"data-attrid\":\"kc:/collection/knowledge_panels/has_phone:phone\"}).div.div.find_all('span')[1].span.text.strip()\n",
    "      print(Fore.GREEN + \"Phone\")\n",
    "    except:\n",
    "      phone = np.nan\n",
    "      print(Fore.RED + \"Phone\")\n",
    "\n",
    "    try:\n",
    "      website = container.find('a', text='Website')['href']\n",
    "      print(Fore.GREEN + \"Website\")\n",
    "    except:\n",
    "      website = np.nan\n",
    "      print(Fore.RED + \"Website\")\n",
    "      \n",
    "    temp = pd.DataFrame({\n",
    "      'title': [title],\n",
    "      'star_count': [star_count],\n",
    "      'review_count': [review_count],\n",
    "      'attributes': [attributes],\n",
    "      'address': [address],\n",
    "      'hours': [hours],\n",
    "      'menu': [menu],\n",
    "      'phone': [phone],\n",
    "      'website':[website],\n",
    "#       'pictures': [pictures],\n",
    "      'city': [self.city],\n",
    "      'state': [self.state]\n",
    "    })\n",
    "\n",
    "    return temp\n",
    "\n",
    "  def search(self):\n",
    "      \"\"\"\n",
    "        Search Google\n",
    "\n",
    "        Args:\n",
    "          query (str): The query to search for\n",
    "\n",
    "        Returns:\n",
    "          list: The search results\n",
    "      \"\"\"\n",
    "      self.counter = 23\n",
    "      #define query and link\n",
    "      for city in self.cities:\n",
    "        self.city=city\n",
    "        query = f\"{self.city}, {self.state} Restaurants\"\n",
    "        print(query)\n",
    "        query = query.replace(\",\", \"%2C\").replace(\" \", \"+\").replace(\" \", \"%20\")\n",
    "        main_link = f\"https://www.google.com/search?q={query}\"\n",
    "\n",
    "        #create driver\n",
    "        self.restaurant_driver = self.__driver('__Thread__')\n",
    "        \n",
    "\n",
    "        @retry((ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError), tries=20, delay=2, backoff=2)\n",
    "        def serve_link(link):\n",
    "          self.restaurant_driver.get(link)\n",
    "        serve_link(main_link)\n",
    "\n",
    "        \n",
    "        #Catch Captcha\n",
    "        try:\n",
    "          source = self.restaurant_driver.page_source\n",
    "          html = BeautifulSoup(source, 'lxml')\n",
    "          # TRY TO FIND MORE RESULTS BUTTON\n",
    "          try:\n",
    "            result = html.find(text='More places').parent.parent.parent['href']\n",
    "          except:\n",
    "            try:\n",
    "              result = html.find(text='More results').parent.parent.parent['href']\n",
    "            except:\n",
    "              result = html.find(text='View all').parent.parent.parent['href']\n",
    "          link = f\"https://www.google.com/{result}\"\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          print(f\"{self.city}, {self.state}\")\n",
    "          #try to find captcha box to see if google is blocking requests\n",
    "          captcha = html.find('div', class_='recaptcha-checkbox-border')\n",
    "          if captcha==True:\n",
    "            breakpoint()\n",
    "          else:\n",
    "            self.restaurant_driver.quit()\n",
    "            continue\n",
    "            \n",
    "\n",
    "        try:\n",
    "          # view all restaurants for location\n",
    "          WebDriverWait(self.restaurant_driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".MXl0lf\"))).click()\n",
    "        except:\n",
    "          self.restaurant_driver.quit()\n",
    "          continue\n",
    "        #serve_link(link)\n",
    "        source = self.restaurant_driver.page_source\n",
    "        html = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        # GRAB MAIN PLACES\n",
    "        self.__main_places(html)\n",
    "\n",
    "        #try to click next button\n",
    "        restaurants_exist = True\n",
    "        while restaurants_exist:\n",
    "          #TEST THIS BLOCK\n",
    "          try:\n",
    "            #wait until the list of restaurants is visible\n",
    "            time.sleep(3)\n",
    "            restaurant_html = BeautifulSoup(self.restaurant_driver.page_source, 'lxml')\n",
    "            restaurants = restaurant_html.find('div', class_='rl_full-list').div.div.div.div.find_next_siblings()[2].find_next('div').parent\n",
    "            #grab all restaurant info\n",
    "            restaurant_length = len(restaurants)\n",
    "          except Exception as e:\n",
    "            print(e)\n",
    "            restaurants_exist = False\n",
    "            break\n",
    "            \n",
    "            \n",
    "          if restaurants_exist==True:\n",
    "            for idx, restaurant in enumerate(restaurants):\n",
    "              #SKIP OVER ADVERTISEMENTS\n",
    "              \n",
    "              try:\n",
    "                if \"Why this ad?\" in restaurant.a.div.find_next('div').find_next_siblings()[0].div.text:\n",
    "                  continue\n",
    "              except:\n",
    "                pass\n",
    "\n",
    "              rest_data = self.__restaurant_data(idx, restaurant)\n",
    "              rest_data['pictures'] = [self.__picture_data(restaurant)]\n",
    "              self.restaurant_dataframe = self.restaurant_dataframe.append(rest_data, ignore_index=True)\n",
    "              \n",
    "            if len(self.restaurant_dataframe)%200==0:\n",
    "              self.counter+=1\n",
    "            try:\n",
    "              restaurant_html = BeautifulSoup(self.restaurant_driver.page_source, 'lxml')\n",
    "              time.sleep(3)\n",
    "              WebDriverWait(self.restaurant_driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.ID, \"pnnext\"))\n",
    "              ).click()\n",
    "            except:\n",
    "              restaurants_exist = False\n",
    "        self.discover_dataframe.to_csv(\"./discover_ct.csv\")\n",
    "        self.restaurant_dataframe.to_csv(\"./ct_restaurants.csv\")\n",
    "        try:\n",
    "          self.restaurant_driver.quit()\n",
    "        except:\n",
    "          pass\n",
    "      return self.discover_dataframe, self.restaurant_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mPicture Link: https://www.google.com/maps/uv?pb=!1s0x89e6828448b05299:0x59d069e23c28a690!3m1!7e115!4shttps://lh5.googleusercontent.com/p/AF1QipMf2Y4et1ZDa1Ws2Qdl0sVf4H0pT48_5SEVvt2I%3Dw520-h350-n-k-no!5sAbington,+Connecticut+Restaurants+-+Google+Search!15zQ2dJZ0FRPT0&imagekey=!1e10!2sAF1QipMf2Y4et1ZDa1Ws2Qdl0sVf4H0pT48_5SEVvt2I&hl=en\n"
     ]
    }
   ],
   "source": [
    "spider = GoogleSpider(state='Connecticut', cities=list(cities))\n",
    "disc, rest = spider.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
